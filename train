#!/usr/bin/env python

from __future__ import print_function

import os
import json
import sys
import time
from argparse import ArgumentParser
from shutil import copy
import traceback
import numpy as np

import torch
from torch.utils.data.sampler import Sampler

from lib.model.faster_rcnn.resnet import resnet
from lib.model.utils.net_utils import adjust_learning_rate
from lib.model.utils.config import cfg
from lib.roi_data_layer.roidb import combined_roidb
from lib.roi_data_layer.roibatchLoader import roibatchLoader


def train(env, mGPUs):

    if env == 'sagemaker':
        prefix = '/opt/ml/'
        model_path = os.path.join(prefix, 'model')
        param_path = os.path.join(prefix, 'input/config/hyperparameters.json')
        output_path = os.path.join(prefix, 'output')

    elif env == 'local':
        model_path = './save/%s/' % str(int(time.time()))
        if not os.path.exists(model_path):
            os.makedirs(model_path)
        param_path = './hyperparameters.json'
        copy(param_path, model_path)
        output_path = '.'
    else:
        raise ValueError

    print('Starting the training.')
    try:
        with open(param_path, 'r') as tc:
            hyperparameters = json.load(tc)

        print('hyperparameters:', hyperparameters)

        batch_size = int(hyperparameters['batch_size'])
        epochs = int(hyperparameters['epochs'])
        lr = float(hyperparameters['lr'])
        imdb_name = hyperparameters['imdb_name']
        num_workers = int(hyperparameters['num_workers'])
        lr_patience = int(hyperparameters['lr_patience'])
        lr_decay_gamma = float(hyperparameters['lr_decay_gamma'])  # 0.1
        early_stopping_patience = float(hyperparameters['patience'])  # 4

        print('Building training dataset...')
        imdb_train, roidb_train, ratio_list_train, ratio_index_train = combined_roidb(imdb_name + '_train')

        dataset_train = roibatchLoader(roidb_train, ratio_list_train,
                                       ratio_index_train, batch_size,
                                       imdb_train.num_classes, training=True)
        dataloader_train = torch.utils.data.DataLoader(dataset_train,
                                                       batch_size=batch_size,
                                                       num_workers=num_workers,
                                                       drop_last=True,)
        print('done.')
        print('-' * 100)

        print('Building validation dataset...')
        imdb_val, roidb_val, ratio_list_val, ratio_index_val = combined_roidb(imdb_name + '_validation', training=False)
        dataset_val = roibatchLoader(roidb_val, ratio_list_val,
                                     ratio_index_val,  batch_size,
                                     imdb_val.num_classes,
                                     training=False)
        dataloader_val = torch.utils.data.DataLoader(dataset_val,
                                                     batch_size=batch_size,
                                                     num_workers=num_workers,
                                                     drop_last=True)
        print('done.')
        print('-' * 100)

        classes = np.asarray(['__background__',
                              'text',
                              'structured_data',
                              'graphical_chart'])

        fasterRCNN = resnet(classes, 'resnet101',
                            pretrained=True, class_agnostic=False)
        fasterRCNN.create_architecture()
        fasterRCNN.cuda()
        if mGPUs:
            fasterRCNN = torch.nn.DataParallel(fasterRCNN)

        params = []
        for key, value in dict(fasterRCNN.named_parameters()).items():
            if value.requires_grad:
                    params += [{'params': [value], 'lr': lr}]

        optimizer = torch.optim.Adam(params)
        best_val_loss = 10000
        if args.load_model:
            print('Loading weights from %s ...' % args.load_model)
            checkpoint = torch.load(args.load_model)
            fasterRCNN.load_state_dict(checkpoint['model'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            best_val_loss = checkpoint.get('best_val_loss', 10000)

        train_log_path = os.path.join(model_path, 'train_log.csv')
        with open(train_log_path, 'a+') as f:
            f.write('epoch,loss,rpn_cls,rpn_box,rcnn_cls,rcnn_box\n')
        val_log_path = os.path.join(model_path, 'val_log.csv')
        with open(val_log_path, 'a+') as f:
            f.write('epoch,loss,rpn_cls,rpn_box,rcnn_cls,rcnn_box\n')

        train_log = []
        val_log = []

        patience_count = 0
        lr_adjust_count = 0

        fasterRCNN.train()

        for epoch in range(epochs):

            train_loss_tot = 0
            train_rpn_cls_tot = 0
            train_rpn_box_tot = 0
            train_rcnn_cls_tot = 0
            train_rcnn_box_tot = 0
            num_examples_train = 0

            train_start_time = time.time()
            for batch_idx, (im_data, im_info, gt_boxes, num_boxes) in enumerate(dataloader_train):

                ok_mask = gt_boxes.sum(dim=1).sum(dim=1) > 0
                if ok_mask.sum() != batch_size:
                    print(f'{ok_mask.sum()}/{batch_size} '
                          f'images in training batch are ok.')

                im_data = im_data.cuda()[ok_mask]
                im_info = im_info.cuda()[ok_mask]
                gt_boxes = gt_boxes.cuda()[ok_mask]
                num_boxes = num_boxes.cuda()[ok_mask]
                current_batch_size = ok_mask.sum()

                (rois, cls_prob,
                 bbox_pred, rpn_loss_cls,
                 rpn_loss_box, RCNN_loss_cls,
                 RCNN_loss_bbox, rois_label) = fasterRCNN(im_data, im_info,
                                                          gt_boxes, num_boxes)

                # Each worker returns the mean losses (rpn/RCNN cls/bbox)
                # for the examples given to them. To get the total mean losses
                # for the entire mini-batch I  average the workers losses
                # together. This is not entirely correct because workers will
                # not always get the same number of examples.
                # It's probably an "ok" approximation.
                batch_rpn_loss_cls = rpn_loss_cls.mean()
                batch_rpn_loss_box = rpn_loss_box.mean()
                batch_RCNN_loss_cls = RCNN_loss_cls.mean()
                batch_RCNN_loss_bbox = RCNN_loss_bbox.mean()

                # Sum the 4 mean losses together to get the final mean loss for
                # this minibatch.
                batch_train_loss = batch_rpn_loss_cls + batch_rpn_loss_box + \
                                   batch_RCNN_loss_cls + batch_RCNN_loss_bbox

                # Update the model
                fasterRCNN.zero_grad()
                optimizer.zero_grad()
                batch_train_loss.backward()
                optimizer.step()

                # Multiply the mean batch losses by the batch size before
                # accumulating the total losses. The total losses are used
                # only for logging purposes.
                num_examples_train += current_batch_size
                train_loss_tot += batch_train_loss.item() * float(current_batch_size)
                train_rpn_cls_tot += batch_rpn_loss_cls.item() * float(current_batch_size)
                train_rpn_box_tot += batch_rpn_loss_box.item() * float(current_batch_size)
                train_rcnn_cls_tot += batch_RCNN_loss_cls.item() * float(current_batch_size)
                train_rcnn_box_tot += batch_RCNN_loss_bbox.item() * float(current_batch_size)

            train_end_time = time.time()

            epoch_log_train = {'epoch': epoch,
                               'loss': train_loss_tot/float(num_examples_train),
                               'rpn_cls': train_rpn_cls_tot/float(num_examples_train),
                               'rpn_box': train_rpn_box_tot/float(num_examples_train),
                               'rcnn_cls': train_rcnn_cls_tot/float(num_examples_train),
                               'rcnn_box': train_rcnn_box_tot/float(num_examples_train)}
            with open(train_log_path, 'a') as f:
                f.write(','.join([str(v) for v in epoch_log_train.values()]) + '\n')

            print("[epoch %2d] train loss: %.4f; "
                  "lr: %.2e;" % (epoch, epoch_log_train['loss'], lr))
            print("\t\t\ttrain time: %f;" % (train_end_time - train_start_time))
            print("\t\t\t"
                  "train rpn_cls: %.4f; "
                  "train rpn_box: %.4f; "
                  "train rcnn_cls: %.4f; "
                  "train rcnn_box: %.4f;"
                  % (epoch_log_train['rpn_cls'],
                     epoch_log_train['rpn_box'],
                     epoch_log_train['rcnn_cls'],
                     epoch_log_train['rcnn_box']))

            train_log.append(epoch_log_train)

            val_loss_tot = 0
            val_rpn_cls_tot = 0
            val_rpn_box_tot = 0
            val_rcnn_cls_tot = 0
            val_rcnn_box_tot = 0
            num_examples_val = 0

            val_start_time = time.time()
            with torch.no_grad():
                for im_data, im_info, gt_boxes, num_boxes in dataloader_val:
                    ok_mask = gt_boxes.sum(dim=1).sum(dim=1) > 0
                    if ok_mask.sum() != batch_size:
                        print(f'{int(ok_mask.sum())}/{batch_size} '
                              f'images in validation batch are ok')
                    im_data = im_data.cuda()[ok_mask]
                    im_info = im_info.cuda()[ok_mask]
                    gt_boxes = gt_boxes.cuda()[ok_mask]
                    num_boxes = num_boxes.cuda()[ok_mask]
                    current_batch_size = ok_mask.sum()

                    (rois, cls_prob,
                     bbox_pred, rpn_loss_cls,
                     rpn_loss_box, RCNN_loss_cls,
                     RCNN_loss_bbox, rois_label) = fasterRCNN(im_data, im_info,
                                                              gt_boxes, num_boxes)

                    batch_rpn_loss_cls = rpn_loss_cls.mean()
                    batch_rpn_loss_box = rpn_loss_box.mean()
                    batch_RCNN_loss_cls = RCNN_loss_cls.mean()
                    batch_RCNN_loss_bbox = RCNN_loss_bbox.mean()

                    batch_val_loss = batch_rpn_loss_cls + batch_rpn_loss_box + \
                                     batch_RCNN_loss_cls + batch_RCNN_loss_bbox

                    num_examples_val += current_batch_size
                    val_loss_tot += batch_val_loss.item() * float(current_batch_size)
                    val_rpn_cls_tot += batch_rpn_loss_cls.item() * float(current_batch_size)
                    val_rpn_box_tot += batch_rpn_loss_box.item() * float(current_batch_size)
                    val_rcnn_cls_tot += batch_RCNN_loss_cls.item() * float(current_batch_size)
                    val_rcnn_box_tot += batch_RCNN_loss_bbox.item() * float(current_batch_size)

            val_end_time = time.time()

            epoch_log_val = {'epoch': epoch,
                             'loss': val_loss_tot / float(num_examples_val),
                             'rpn_cls': val_rpn_cls_tot / float(num_examples_val),
                             'rpn_box': val_rpn_box_tot / float(num_examples_val),
                             'rcnn_cls': val_rcnn_cls_tot / float(num_examples_val),
                             'rcnn_box': val_rcnn_box_tot / float(num_examples_val)}

            with open(val_log_path, 'a') as f:
                f.write(','.join([str(v) for v in epoch_log_val.values()]) + '\n')

            print("[epoch %2d] val loss: %.4f; "
                  "lr: %.2e;" % (epoch, epoch_log_val['loss'], lr))
            print("\t\t\tval time: %f;" % (val_end_time - val_start_time))
            print("\t\t\tval rpn_cls: %.4f; "
                  "val rpn_box: %.4f; "
                  "val rcnn_cls: %.4f; "
                  "val rcnn_box: %.4f;"
                  % (epoch_log_val['rpn_cls'],
                     epoch_log_val['rpn_box'],
                     epoch_log_val['rcnn_cls'],
                     epoch_log_val['rcnn_box']))

            val_log.append(epoch_log_val)

            if epoch_log_val['loss'] < best_val_loss:
                patience_count = 0

                print('val loss improved from %.4f to %.4f' % (best_val_loss, epoch_log_val['loss']))
                best_val_loss = epoch_log_val['loss']
                model_file_path = os.path.join(model_path, 'faster-rcnn.pt')
                torch.save({
                    'epoch': epoch + 1,
                    'model': fasterRCNN.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'best_val_loss': best_val_loss,
                    'classes': classes
                }, model_file_path)

                model_file_size = os.path.getsize(model_file_path)

                print("saved model file:", model_file_path)
                print('model file size:', model_file_size)
            else:
                patience_count += 1

            if patience_count >= lr_patience and lr_adjust_count < 3:
                adjust_learning_rate(optimizer, lr_decay_gamma)
                lr_adjust_count += 1
                lr *= lr_decay_gamma
            if patience_count == early_stopping_patience:
                return

    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)


if __name__ == '__main__':
    parser = ArgumentParser()
    parser.add_argument('--env', dest='env', help='training environment (local or sagemaker)', default='sagemaker', type=str)
    parser.add_argument('--load_model', help='path to model file to load for resuming training', type=str)
    parser.add_argument('--cfg', help='Optional config file', type=str)
    # parser.add_argument('--mGPUs', dest='mGPUs',
    #                     help='whether use multiple GPUs',
    #                     action='store_true')

    args = parser.parse_args()

    cfg.TRAIN.USE_FLIPPED = False
    train(args.env, True)

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
